%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Long Book Title
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@string{long-jour-jasa   = "Journal of the American Statistical Association (JASA)"}
@string{long-jour-jcgs   = "Journal of Computational and Graphical Statistics (JCGS)"}
@string{long-jour-cjs    = "Canadian Journal of Statistics (CJS)"}
@string{long-jour-aos    = "The Annals of Statistics"}
@string{long-jour-ss     = "Statistica Sinica"}
long-
@string{long-NeurIPS = "Advances in Neural Information Processing Systems (NeurIPS)"}
@string{long-AIStats = "Proceedings of the Workshop on Artificial Intelligence and Statistics (AI-Stats)"}
@string{long-ICML = "Proceedings of the International Conference on Machine Learning (ICML)"}
@string{long-ECML = "Proceedings of the European Conference on Machine Learning (ECML)"}
@string{long-UAI  = "Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI)"}
@string{long-COLT = "Proceedings of the Conference on Computational Learning Theory (COLT)"}
@string{long-ACL  = "Proceedings of the Conference of the Association for Computational Linguistics (ACL)"}
@string{long-EACL = "Proceedings of the Conference of the European Association for Computational Linguistics (EACL)"}
@string{long-FAT = "Proceedings of the Conference on Fairness, Accountability and Transparency (FAT)"}
@string{long-LREC = "Proceedings of the Conference on Language Resources and Evaluation (LREC)"}
@string{long-NAACL= "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)"}
@string{long-NAACLHLT= "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technology (NAACL/HLT)"}
@string{long-HLTEMNLP= "Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP)"}
@string{long-TACL=  "Transactions of the Association for Computational Linguistics (TACL)"}
@string{long-EMNLP = "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)"}
@string{long-COLING = "Proceedings of the International Conference on Computational Linguistics (COLING)"}
@string{long-COLINGACL = "Proceedings of the Joint International Conference on Computational Linguistics and Association of Computational Linguistics (COLING/ACL)"}
@string{long-CIKM = "Processing of the Conference on Information and Knowledge Management (CIKM)"}
@string{long-SIGIR = "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR)"}
@string{long-IJCAI = "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)"}
@string{long-ICASSP= "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"}
@string{long-CVPR  = "Computer Vision and Pattern Recognition (CVPR)"}
@string{long-ICCV  = "International Conference on Computer Vision (ICCV)"}
@string{long-IWPT  = "International Workshop on Parsing Technologies (IWPT)"}
@string{long-AMTA  = "Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA)"}
@string{long-KDD   = "Conference on Knowledge Discovery and Data Mining (KDD)"}
@string{long-CHI   = "Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems (CHI)"}
long-
@string{long-JMLR  = "Journal of Machine Learning Research (JMLR)"}
@string{long-JAIR  = "Journal of Artificial Intelligence Research (JAIR)"}
@string{long-ML    = "Machine Learning Journal (ML)"}
@string{long-CL    = "Computational Linguistics"}
@string{long-CONLL = "Proceedings of the Conference on Natural Language Learning (CoNLL)"}
@string{long-AAAI  = "Proceedings of the National Conference on Artificial Intelligence (AAAI)"}
@string{long-SODA  = "Symposium on Discrete Algorithms (SODA)"}
@string{long-STOC  = "Symposium on the Theory of Computing (STOC)"}
@string{long-FOCS  = "IEEE Symposium on Foundations of Computer Science (FOCS)"}
@string{long-ICSLP = "International Conference on Spoken Language Processing (ICSLP)"}
@string{long-IJCNLP= "Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP)"}
@string{long-WSDM  = "Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM)"}
@string{long-ICLR  = "The International Conference on Learning Representations (ICLR)"}
@string{long-NEURIPS = "Advances in Neural Information Processing Systems (NeurIPS)"}
@string{long-PREPRINT = "Preprint"}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Short Book Title
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@string{jour-jasa   = "JASA)"}
@string{jour-jcgs   = "JCGS)"}
@string{jour-cjs    = "CJS)"}
@string{jour-aos    = "Annals of Stats."}
@string{jour-ss     = "Statistica Sinica"}
@string{NeurIPS = "NeurIPS"}
@string{AIStats = "AI-Stats"}
@string{ICML = "ICML"}
@string{ECML = "ECML"}
@string{UAI  = "UAI"}
@string{COLT = "COLT"}
@string{ICLR = "ICLR"}
@string{ACL  = "ACL"}
@string{EACL = "EACL"}
@string{LREC = "LREC"}
@string{NAACL= "NAACL"}
@string{NAACLHLT= "NAACL/HLT"}
@string{HLTEMNLP= "HLT/EMNLP"}
@string{TACL=  "TACL"}
@string{FAT = "FAT"}
@string{EMNLP = "EMNLP"}
@string{COLING = "COLING"}
@string{COLINGACL = "COLING/ACL"}
@string{CIKM = "CIKM"}
@string{SIGIR = "SIGIR"}
@string{IJCAI = "IJCAI"}
@string{ICASSP= "ICASSP"}
@string{CVPR  = "CVPR"}
@string{ICCV  = "ICCV"}
@string{IWPT  = "IWPT"}
@string{AMTA  = "AMTA"}
@string{KDD   = "KDD"}
@string{CHI   = "CHI"}
@string{JMLR  = "JMLR"}
@string{JAIR  = "JAIR"}
@string{ML    = "ML"}
@string{CL    = "CL"}
@string{CONLL = "CoNLL"}
@string{AAAI  = "AAAI"}
@string{SODA  = "SODA"}
@string{STOC  = "STOC"}
@string{FOCS  = "FOCS"}
@string{ICSLP = "ICSLP"}
@string{IJCNLP= "IJCNLP"}
@string{WSDM  = "WSDM"}
@string{ICLR  = "ICLR"}
@string{NEURIPS = "NeurIPS"}
@string{PREPRINT = "Preprint"}

% Links
% https://gist.github.com/moewew/27384a6c5f73d202653260236f375a1f

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Publications 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@phdthesis{brantley2021expert,
  title={Expert-in-the-Loop for Sequential Decisions and Predictions},
  shorttitle="",
  author={Brantley, Kianté},
  year={2021},
  school={University of Maryland, College Park},
  url={https://www.proquest.com/openview/7d9dc57392258849ca14e0bdcc6d66e2/1?pq-origsite=gscholar&cbl=18750&diss=y},
}

@mastersthesis{brantley2016bcap,
  title={BCAP: An Artificial Neural Network Pruning Technique to Reduce Overfitting},
  shorttitle="",
  author={Brantley, Kianté},
  year={2016},
  school={University of Maryland, Baltimore County},
  url={https://www.proquest.com/openview/aca94379da7bcfe9975298f08d9df648/1?pq-origsite=gscholar&cbl=18750},
  note={}
}

@inproceedings{brantley2024ranking,
  title={Ranking with Long-Term Constraints},
  author={Brantley, Kianté and Fang, Zhichong and Dean, Sarah and Joachims, Thorsten},
  booktitle=long-WSDM,
  shorttitle=WSDM,
  pages={47--56},
  year={2024},
  abstract={The feedback that users provide through their choices (e.g., clicks, purchases) is one of the most common types of data readily avail able for training search and recommendation algorithms. However, myopically training systems based on choice data may only improve short-term engagement, but not the long-term sustainability of the platform and the long-term benefits to its users, content providers, and other stakeholders. In this paper, we thus develop a new framework in which decision makers (e.g., platform operators, regulators, users) can express long-term goals for the behavior of the platform (e.g., fairness, revenue distribution, legal requirements). These goals take the form of exposure or impact targets that go well beyond individual sessions, and we provide new control-based algorithms to achieve these goals. In particular, the controllers are designed to achieve the stated long-term goals with minimum impact on shortterm engagement. Beyond the principled theoretical derivation of the controllers, we evaluate the algorithms on both synthetic and real-world data. While all controllers perform well, we find that they provide interesting trade-offs in efficiency, robustness, and the ability to plan ahead.},
  url={https://arxiv.org/abs/2307.04923},
  note={}
}

@inproceedings{brantley2023interactive,
  title={Interactive Text Generation},
  author={Faltings, Felix and Galley, Michel and Brantley, Kianté and Peng, Baolin and Cai, Weixin and Zhang, Yizhe and Gao, Jianfeng and Dolan, William B},
  booktitle=long-EMNLP,
  shorttitle=EMNLP,
  pages={4450--4468},
  year={2023},
  abstract={Users interact with text, image, code, or other editors on a daily basis. However, machine learning models are rarely trained in the settings that reflect the interactivity between users and their editor. This is understandable as training AI models with real users is not only slow and costly, but what these models learn may be specific to user interface design choices. Unfortunately, this means most of the research on text, code, and image generation has focused on non-interactive settings, whereby the model is expected to get everything right without ac- counting for any input from a user who may be willing to help. We introduce a new Interactive Text Generation task that allows training generation models interactively without the costs of involving real users, by using user simulators that provide edits that guide the model towards a given target text. We train our interactive models using Imitation Learning, and our ex- periments against competitive non-interactive generation models show that models trained interactively are superior to their non-interactive counterparts, even when all models are given the same budget of user inputs or edits.},
  url={https://arxiv.org/abs/2303.00908},
  note={}
}

@inproceedings{brantley2023reinforcement,
  title={Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization},
  author={Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kianté and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  booktitle=long-ICLR,
  shorttitle=ICLR,
  year={2023},
  abstract={We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of opensource libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs, for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al., 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al., 2017)), based on both automatic and human evaluations.},
  url={https://arxiv.org/abs/2210.01241},
  note={Spotlight},
}

@inproceedings{brantley2024coactive,
  title={Coactive Learning for Large Language Models using Implicit User Feedback},
  author={Tucker, Aaron David and Brantley, Kianté and Cahall, Adam and Joachims, Thorsten},
  booktitle=long-ICML,
  shorttitle=ICML,
  year={2024},
  abstract={We propose coactive learning as a model and feedback mechanism for training large language models (LLMs). The key insight is that users provide implicit feedback whenever they edit the text $\y$ proposed by an LLM. While the edited text $\ybar$ is typically not a gold-standard example for supervised training, coactive learning merely requires that the edited text $\ybar$ is an improvement over the proposed text $\y$. Note that such weak implicit preference feedback $\ybar \succ \y$ is available in many application settings on a per-user basis, thus enabling the personalization of LLMs. In this paper, we develop the theoretical basis for coactive training of non-linear models, and we derive CoRLL as the first coactive learning algorithm for LLMs. Empirical results indicate that CoRLL is effective even for weak and noisy coactive preference feedback, making it a promising algorithm for training and personalization of LLMs from feedback that is naturally collected in many use cases.},
  url={https://www.cs.cornell.edu/people/tj/publications/tucker_etal_24a.pdf},
  keywords={recent, representative},
}

@inproceedings{brantley2024adversarial,
  title={Adversarial Imitation Learning via Boosting},
  author={Chang, Jonathan Daniel and Sreenivas, Dhruv and Huang, Yingbing and Brantley, Kianté and Sun, Wen},
  booktitle=long-ICLR,
  shorttitle=ICLR,
  year={2024},
  abstract={Adversarial imitation learning (AIL) has stood out as a dominant framework across various imitation learning (IL) applications, with Discriminator Actor Critic (DAC) demonstrating the effectiveness of off-policy learning algorithms in improving sample efficiency and scalability to higher-dimensional observations. Despite DAC’s empirical success, the original AIL objective is on-policy and DAC’s ad-hoc application of off-policy training does not guarantee successful imitation. Follow-up work such as ValueDICE tackles this issue by deriving a fully off-policy AIL objective. Instead in this work, we develop a novel and principled AIL algorithm via the framework of boosting. Like boosting, our new algorithm, AILBoost, maintains an ensemble of weighted weak learners (i.e., policies) and trains a discriminator that witnesses the maximum discrepancy between the distributions of the ensemble and the expert policy. We maintain a weighted replay buffer to represent the state-action distribution induced by the ensemble, allowing us to train discriminators using the entire data collected so far. Empirically, we evaluate our algorithm on both controller state-based and pixel-based environments from the DeepMind Control Suite. AILBoost outperforms DAC on both types of environments, demonstrating the benefit of properly weighting replay buffer data for off-policy training. On state-based environments, AILBoost outperforms ValueDICE and IQ-Learn, achieving state-of-the-art performance with as little as one expert trajectory.},
  url={https://arxiv.org/abs/2404.08513},
}

@inproceedings{brantley2023policy,
  title={Policy-Gradient Training of Language Models for Ranking},
  author={Gao, Ge and Chang, Jonathan Daniel and Cardie, Claire and Brantley, Kianté and Joachims, Thorsten},
  booktitle={NeurIPS 2023 Workshop Foundation Models for Decision Making Workshop},
  shorttitle={NeurIPS 2023 Workshop Foundation Models for Decision Making Workshop},
  year={2023},
  abstract={Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems via policy gradient, with little reliance on complex heuristics, and it effectively unifies the training objective with downstream decision-making quality. We conduct extensive experiments on various text retrieval benchmarks. The results demonstrate that when the training objective aligns with the evaluation setup, Neural PG-RANK yields remarkable in-domain performance improvement, with substantial out-of-domain generalization to some critical datasets employed in downstream question answering tasks.},
  url={https://arxiv.org/abs/2310.04407},
  keywords={workshop},
}

@inproceedings{brantley2023learning,
  title={Learning to Generate Better Than Your LLM},
  author={Chang, Jonathan and Brantley, Kianté and Ramamurthy, Rajkumar, and Misra, Dipendra, and Sun, Wen},
  booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  shorttitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  year={2023},
  abstract={Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for text generation. In particular, recent LLMs such as ChatGPT and GPT4 can engage in fluent conversations with users after finetuning with RL. Capitalizing on key properties of text generation, we seek to investigate RL algorithms beyond general purpose algorithms like Proximal Policy Optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We provide two ways for the guide LLM to interact with the LLM to be optimized for maximizing rewards. The guide LLM can generate text which serves as additional starting states for the RL optimization procedure. The guide LLM can also be used to complete the partial sentences generated by the LLM that is being optimized, treating the guide LLM as an expert to imitate and surpass eventually. We experiment on the IMDB positive sentiment, CommonGen, and TL;DR summarization tasks. We show that our RL algorithms achieve higher performance than supervised learning (SL) and the RL baseline PPO, demonstrating the benefit of interaction with the guide LLM. On both CommonGen and TL;DR, we not only outperform our SL baselines but also improve upon PPO across a variety of metrics beyond the one we optimized for. Our code can be found at https://github.com/Cornell-RL/tril.},
  url={https://arxiv.org/abs/2306.11816},
  keywords={workshop, representative},
}

@inproceedings{brantley2023lilgym,
  title={lilGym: Natural Language Visual Reasoning with Reinforcement Learning},
  author={Wu, Anne and Brantley, Kianté and Kojima, Noriyuki and Artzi, Yoav},
  booktitle=long-ACL,
  shorttitle=ACL,
  year={2023},
  abstract={We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation in every possible world state by annotating all statements with executable Python programs. Each statement is paired with multiple start states and reward functions to form thousands of distinct Markov Decision Processes of varying difficulty. We experiment with lilGym with different models and learning regimes. Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem. lilGym is available at https://lil.nlp.cornell.edu/lilgym/.},
  url={https://arxiv.org/abs/2211.01994},
}
}

@inproceedings{brantley2021successor,
  title={Successor feature sets: Generalizing successor representations across policies},
  author={Brantley, Kianté and Mehri, Soroush and Gordon, Geoff J},
  booktitle=long-AAAI,
  shorttitle=AAAI,
  volume={35},
  number={13},
  pages={11774--11781},
  year={2021},
  abstract={Successor-style representations have many advantages for reinforcement learning: for example, they can help an agent generalize from past experience to new goals, and they have been proposed as explanations of behavioral and neural data from human and animal learners. They also form a natural bridge between model-based and model-free RL methods: like the former they make predictions about future experiences, and like the latter they allow efficient prediction of total discounted rewards. However, successor-style representations are not optimized to generalize across policies: typically, we maintain a limited-length list of policies, and share information among them by representation learning or GPI\@. Successor-style representations also typically make no provision for gathering information or reasoning about latent variables. To address these limitations, we bring together ideas from predictive state representations, belief space value iteration, successor features, and convex analysis: we develop a new, general successor-style representation, together with a Bellman equation that connects multiple sources of information within this representation, including different latent states, policies, and reward functions. The new representation is highly expressive: for example, it lets us efficiently read off an optimal policy for a new reward function, or a policy that imitates a new demonstration. For this paper, we focus on exact computation of the new representation in small, known environments, since even this restricted setting offers plenty of interesting questions. Our implementation does not scale to large, unknown environments --- nor would we expect it to, since it generalizes POMDP value iteration, which is difficult to scale. However, we believe that future work will allow us to extend our ideas to approximate reasoning in large, unknown environments. We conduct experiments to explore which of the potential barriers to scaling are most pressing.},
  url={https://arxiv.org/abs/2103.02650},
}

@inproceedings{brantley2020active,
  title={Active Imitation Learning with Noisy Guidance},
  author={Brantley, Kianté and Sharaf, Amr and Daumé III, Hal},
  booktitle=long-ACL,
  shorttitle=ACL,
  pages={2093--2105},
  year={2020},
  abstract={Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies. Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical. To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance. Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary. We apply LEAQI to three sequence labeling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.},
  url={https://arxiv.org/abs/2005.12801},
}

@inproceedings{brantley2020constrained,
  title={Constrained episodic reinforcement learning in concave-convex and knapsack settings},
  author={Brantley, Kianté and Dudik, Miro and Lykouris, Thodoris and Miryoosefi, Sobhan and Simchowitz, Max and Slivkins, Aleksandrs and Sun, Wen},
  booktitle=long-NEURIPS,
  shorttitle=NEURIPS,
  volume={33},
  pages={16315--16326},
  year={2020},
  abstract={We propose an algorithm for tabular episodic reinforcement learning with constraints. We provide a modular analysis with strong theoretical guarantees for settings with concave rewards and convex constraints, and for settings with hard constraints (knapsacks). Most of the previous work in constrained reinforcement learning is limited to linear constraints, and the remaining work focuses on either the feasibility question or settings with a single episode. Our experiments demonstrate that the proposed algorithm significantly outperforms these approaches in existing constrained episodic environments.},
  url={https://arxiv.org/abs/2006.05051},
}

@inproceedings{brantley2019disagreement,
  title={Disagreement-regularized imitation learning},
  author={Brantley, Kianté and Sun, Wen and Henaff, Mikael},
  booktitle=long-ICLR,
  shorttitle=ICLR,
  year={2019},
  abstract={We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.},
  note={Spotlight},
  url={https://openreview.net/pdf?id=rkgbYyHtwB},
  keywords={representative}
}

@inproceedings{brantley2019non,
  title={Non-monotonic sequential text generation},
  author={Welleck, Sean and Brantley, Kianté and Iii, Hal Daumé and Cho, Kyunghyun},
  booktitle=long-ICML,
  shorttitle=ICML,
  pages={6716--6726},
  year={2019},
  organization={PMLR},
  abstract={Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy’s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.},
  url={https://arxiv.org/abs/1902.02192},
}

@inproceedings{brantley2019reinforcement,
  title={Reinforcement learning with convex constraints},
  author={Miryoosefi, Sobhan and Brantley, Kianté and Daumé III, Hal and Dudik, Miro and Schapire, Robert E},
  booktitle=long-NEURIPS,
  shorttitle=NEURIPS,
  volume={32},
  year={2019},
  abstract={In standard reinforcement learning (RL), a learning agent seeks to optimize the overall reward. However, many key aspects of a desired behavior are more naturally expressed as constraints. For instance, the designer may want to limit the use of unsafe actions, increase the diversity of trajectories to enable exploration, or approximate expert trajectories when rewards are sparse. In this paper, we propose an algorithmic scheme that can handle a wide class of constraints in RL tasks: specifically, any constraints that require expected values of some vector measurements (such as the use of an action) to lie in a convex set. This captures previously studied constraints (such as safety and proximity to an expert), but also enables new classes of constraints (such as diversity). Our approach comes with rigorous theoretical guarantees and only relies on the ability to approximately solve standard RL tasks. As a result, it can be easily adapted to work with any model-free or model-based RL. In our experiments, we show that it matches previous algorithms that enforce safety via constraints, but can also enforce new properties that these algorithms do not incorporate, such as diversity.},
  url={https://arxiv.org/abs/1906.09323},
}

@inproceedings{brantley2017umd,
  title={The UMD Neural Machine Translation Systems at WMT17 Bandit Learning Task},
  author={Sharaf, Amr and Feng, Shi and Nguyen, Khanh and Brantley, Kianté and Daumé III, Hal},
  booktitle={Proceedings of the Second Conference on Machine Translation},
  shorttitle={Proceedings of the Second Conference on Machine Translation},
  pages={667--673},
  year={2017},
  abstract={We describe the University of Maryland machine translation systems submitted to the WMT17 German-English Bandit Learning Task. The task is to adapt a translation system to a new domain, using only bandit feedback: the system receives a German sentence to translate, produces an English sentence, and only gets a scalar score as feedback. Targeting these two challenges (adaptation and bandit learning), we built a standard neural machine translation system and extended it in two ways: (1) robust reinforcement learning techniques to learn effectively from the bandit feedback, and (2) domain adaptation using data selection from a large corpus of parallel data.},
  url={https://aclanthology.org/W17-4778/},
  keywords={workshop}
}

@inproceedings{brantley2015ldaexplore,
  title={Ldaexplore: Visualizing topic models generated using latent dirichlet allocation},
  author={Ganesan, Ashwinkumar and Brantley, Kianté and Pan, Shimei and Chen, Jian},
  booktitle={extvis Workshop - Intelligent User Interfaces (IUI)},
  shorttitle={extvis Workshop - Intelligent User Interfaces (IUI)},
  year={2015},
  abstract={We present LDAExplore, a tool to visualize topic distributions in a given document corpus that are generated using Topic Modeling methods. Latent Dirichlet Allocation (LDA) is one of the basic methods that is predominantly used to generate topics. One of the problems with methods like LDA is that users who apply them may not understand the topics that are generated. Also, users may find it difficult to search correlated topics and correlated documents. LDAExplore, tries to alleviate these problems by visualizing topic and word distributions generated from the document corpus and allowing the user to interact with them. The system is designed for users, who have minimal knowledge of LDA or Topic Modelling methods. To evaluate our design, we run a pilot study which uses the abstracts of 322 Information Visualization papers, where every abstract is considered a document. The topics generated are then explored by users. The results show that users are able to find correlated documents and group them based on topics that are similar.},
  url={https://arxiv.org/abs/1507.06593},
  keywords={workshop},
}

@inproceedings{brantley2024transfer,
  title={When is Transfer Learning Possible?},
  author={Phan, My and Brantley, Kianté and Milani, Stephanie and Mehri, Soroush and Swamy, Gokul and Gordon, Geoffrey J},
  booktitle=long-ICML,
  shorttitle=ICML,
  year={2024},
  abstract={We present a general framework for transfer learning that is flexible enough to capture transfer in supervised, reinforcement, and imitation learning. Our framework enables new insights into the fundamental question of when we can successfully transfer learned information across problems. We model the learner as interacting with a sequence of problem instances, or environments, each of which is generated from a common structural causal model (SCM) by choosing the SCM’s parameters from restricted sets. We derive a procedure that can propagate restrictions on SCM parameters through the SCM’s graph structure to other parameters that we are trying to learn. The propagated restrictions then enable more efficient learning (i.e., transfer). By analyzing the procedure, we are able to challenge widely-held beliefs about transfer learning. First, we show that having sparse changes across environments is neither necessary nor sufficient for transfer. Second, we show an example where the common heuristic of freezing a layer in a network causes poor transfer performance. We then use our procedure to select a more refined set of parameters to freeze, leading to successful transfer learning.},
  url={https://openreview.net/pdf?id=9yADTDHgGu},
}

@inproceedings{brantley2024rebel,
  title={REBEL: Reinforcement Learning via Regressing Relative Rewards},
  author={Gao, Zhaolin and Chang, Jonathan D and Zhan, Wenhao and Oertell, Owen and Swamy, Gokul and Brantley, Kianté and Joachims, Thorsten and Bagnell, J Andrew and Lee, Jason D and Sun, Wen},
  booktitle=PREPRINT,
  shorttitle=PREPRINT,
  year={2024},
  abstract={While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping) and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative rewards via a direct policy parameterization between two completions to a prompt, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally tractable than PPO.},
  url={https://arxiv.org/abs/2404.16767},
  keywords={recent, representative}
}

@inproceedings{brantley2024dataset,
  title={Dataset Reset Policy Optimization for RLHF},
  author={Chang, Jonathan D and Shan, Wenhao and Oertell, Owen and Brantley, Kianté and Misra, Dipendra and Lee, Jason D and Sun, Wen},
  booktitle=PREPRINT,
  shorttitle=PREPRINT,
  year={2024},
  abstract={Reinforcement Learning (RL) from Human Preference-based feedback is a popular paradigm for fine-tuning generative models, which has produced impressive models such as GPT-4 and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new RLHF algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR-PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR summarization and the Anthropic Helpful Harmful (HH) dataset, the generation from DR-PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of GPT4 win-rate. Code for this work can be found at https://github.com/Cornell-RL/drpo.}, 
  url={https://arxiv.org/abs/2404.08495},
  keywords={preprint, recent}
}

@inproceedings{brantley2024rl,
  title={RL for Consistency Models: Faster Reward Guided Text-to-Image Generation},
  author={Oertell, Owen and Chang, Jonathan D and Zhang, Yiyi and Brantley, Kianté and Sun, Wen},
  booktitle=PREPRINT,
  shorttitle=PREPRINT,
  year={2024},
  abstract="Reinforcement learning (RL) has improved guided image generation with diffusion models by directly optimizing rewards that capture image quality, aesthetics, and instruction following capabilities. However, the resulting generative policies inherit the same iterative sampling process of diffusion models that causes slow generation. To overcome this limitation, consistency models proposed learning a new class of generative models that directly map noise to data, resulting in a model that can generate an image in as few as one sampling iteration. In this work, to optimize text-to-image generative models for task specific rewards and enable fast training and inference, we propose a framework for fine-tuning consistency models via RL. Our framework, called Reinforcement Learning for Consistency Model (RLCM), frames the iterative inference process of a consistency model as an RL procedure. RLCM improves upon RL fine-tuned diffusion models on text-to-image generation capabilities and trades computation during inference time for sample quality. Experimentally, we show that RLCM can adapt text-to-image consistency models to objectives that are challenging to express with prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Comparing to RL finetuned diffusion models, RLCM trains significantly faster, improves the quality of the generation measured under the reward objectives, and speeds up the inference procedure by generating high quality images with as few as two inference steps. Our code is available at https://rlcm.owenoertell.com",
  url={https://arxiv.org/abs/2404.03673},
  keywords={preprint, recent}
}

@inproceedings{brantley2024surprising,
  title={A Surprising Failure? Multimodal LLMs and the NLVR Challenge},
  author={Wu, Anne and Brantley, Kianté and Artzi, Yoav},
  booktitle=PREPRINT,
  shorttitle=PREPRINT,
  year={2024},
  abstract={This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and the open-source model IDEFICS -- on the compositional natural language vision reasoning task NLVR. Given a human-written sentence paired with a synthetic image, this task requires the model to determine the truth value of the sentence with respect to the image. Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.},
  url={https://arxiv.org/abs/2402.17793},
  keywords={preprint}
}

@inproceedings{gao2024reviewer2,
  title={Reviewer2: Optimizing Review Generation Through Prompt Generation},
  author={Gao, Zhaolin and Brantley, Kianté and Joachims, Thorsten},
  booktitle=PREPRINT,
  shorttitle=PREPRINT,
  year={2024},
  abstract={Recent developments in LLMs offer new opportunities for assisting authors in improving their work. In this paper, we envision a use case where authors can receive LLM-generated reviews that uncover weak points in the current draft. While initial methods for automated review generation already exist, these methods tend to produce reviews that lack detail, and they do not cover the range of opinions that human reviewers produce. To address this shortcoming, we propose an efficient two-stage review generation framework called Reviewer2. Unlike prior work, this approach explicitly models the distribution of possible aspects that the review may address. We show that this leads to more detailed reviews that better cover the range of aspects that human reviewers identify in the draft. As part of the research, we generate a large-scale review dataset of 27k papers and 99k reviews that we annotate with aspect prompts, which we make available as a resource for future research.},
  url={https://arxiv.org/abs/2402.10886},
  keywords={preprint}
}